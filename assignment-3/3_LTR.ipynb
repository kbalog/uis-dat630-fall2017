{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 - Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "API = \"http://gustav1.ux.uis.no:5002\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_queries(query_file):\n",
    "    queries = {}\n",
    "    with open(query_file, \"r\") as fin:\n",
    "        for line in fin.readlines():\n",
    "            qid, query = line.strip().split(\" \", 1)\n",
    "            queries[qid] = query\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading qrels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_qrels(qrels_file):\n",
    "    qrels = {}\n",
    "    with open(qrels_file, \"r\") as fin:\n",
    "        i = 0\n",
    "        for line in fin.readlines():\n",
    "            i += 1\n",
    "            if i == 1:  # skip header line\n",
    "                continue\n",
    "            qid, doc_id, rel = line.strip().split(\",\", 2)\n",
    "            if qid not in qrels:\n",
    "                qrels[qid] = {}\n",
    "            qrels[qid][doc_id] = rel\n",
    "    return qrels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading features file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_features(features_file):\n",
    "    X, y, qids, doc_ids = [], [], [], []\n",
    "    with open(features_file, \"r\") as f:\n",
    "        i, s_qid = 0, None\n",
    "        for line in f:\n",
    "            items = line.strip().split()\n",
    "            label = int(items[0])\n",
    "            qid = items[1]\n",
    "            doc_id = items[2]\n",
    "            features = np.array([float(i.split(\":\")[1]) for i in items[3:]])\n",
    "            X.append(features)\n",
    "            y.append(label)\n",
    "            qids.append(qid)\n",
    "            doc_ids.append(doc_id)\n",
    "\n",
    "    return X, y, qids, doc_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Issuing a search query againt the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search(indexname, query, field, size=10):\n",
    "    url = \"/\".join([API, indexname, \"_search\"]) + \"?\" \\\n",
    "          + urllib.parse.urlencode({\"q\": query, \"df\": field, \"size\": size})\n",
    "    response = requests.get(url).text\n",
    "    return json.loads(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get term vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def term_vectors(indexname, doc_id, term_statistics=False):\n",
    "    \"\"\"\n",
    "    param term_statistics: Boolean; True iff term_statistics are required.\n",
    "    \"\"\"\n",
    "    url = \"/\".join([API, indexname, doc_id, \"_termvectors\"]) + \"?\" \\\n",
    "          + urllib.parse.urlencode({\"term_statistics\": str(term_statistics).lower()})\n",
    "    response = requests.get(url).text\n",
    "\n",
    "    return json.loads(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze query (return a list of index terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def analyze_query(indexname, query):\n",
    "    url = \"/\".join([API, indexname, \"_analyze\"]) + \"?\" \\\n",
    "          + urllib.parse.urlencode({\"text\": query})\n",
    "    response = requests.get(url).text\n",
    "    r = json.loads(response)\n",
    "    return [t[\"token\"] for t in r[\"tokens\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pointwise LTR class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PointWiseLTRModel(object):\n",
    "    def __init__(self, regressor):\n",
    "        \"\"\"\n",
    "        :param classifier: an instance of scikit-learn regressor\n",
    "        \"\"\"\n",
    "        self.regressor = regressor\n",
    "\n",
    "    def _train(self, X, y):\n",
    "        \"\"\"\n",
    "        Trains and LTR model.\n",
    "        :param X: features of training instances\n",
    "        :param y: relevance assessments of training instances\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        assert self.regressor is not None\n",
    "        self.model = self.regressor.fit(X, y)\n",
    "\n",
    "    def rank(self, ft, doc_ids):\n",
    "        \"\"\"\n",
    "        Predicts relevance labels and rank documents for a given query\n",
    "        :param ft: a list of features for query-doc pairs\n",
    "        :param ft: a list of document ids\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        assert self.model is not None\n",
    "        rel_labels = self.model.predict(ft)\n",
    "        sort_indices = np.argsort(rel_labels)[::-1]\n",
    "\n",
    "        results = []\n",
    "        for i in sort_indices:\n",
    "            results.append((doc_ids[i], rel_labels[i]))\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total number of candidate documents to retrieve from Elasticsearch. Should not be set higher than 200 (otherwise things get unreasonably slow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_DOCS = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total number of features (features will be indexed 1..NUM_FEAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_FEAT = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing the feature vectors for a given query\n",
    "\n",
    "  - We retrieve top `NUM_DOCS` documents for each field (title, content, anchors).\n",
    "  - We ignore those docs that don't have a score in the content field. This also serves as a simple and pragmatic way of filtering out docs that are not in ClueWeb Category B. \n",
    "  \n",
    "**IMPORTANT NOTE** If you compute the BM25 title and anchor scores this way, you will end up with a lot of missing features, which will likely hurt your performance. For IR, there should be no missing features, as matching the query against a field always produces a retrieval score (which might be 0). Therefore, instead of what is done below for Features #2 and #3, you should get the top 200 docs using BM25 content, then compute the BM25 title and anchors based on the termvectors.  Similarly, for the additional features (LM, TFIDF, etc.), compute the retrieval scores for the content, title, and anchors fields yourself based on the termvectors.\n",
    "  \n",
    "This function is used both both when training and when applying the model. When training, the target relevance labels will need to be assigned to each document. That is done in `get_training_data()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_features(qid, query):\n",
    "    feats = {}\n",
    "    print(\"Getting features for query #{} '{}'\".format(qid, query))                \n",
    "\n",
    "    # Analyze query (will be needed for some features)\n",
    "    qterms = analyze_query(\"clueweb12b\", query)\n",
    "    \n",
    "    # Feature 1: BM25 content score\n",
    "    res1 = search(\"clueweb12b\", query, \"content\", size=NUM_DOCS)\n",
    "    # Initializing feature vector with values for Feature 1\n",
    "    print(\"\\tElasticsearch content field ...\")\n",
    "    for doc in res1.get('hits', {}).get(\"hits\", {}):\n",
    "        doc_id = doc.get(\"_id\")\n",
    "        feats[doc_id] = {1: doc.get(\"_score\")}\n",
    "        \n",
    "    # Feature 2: BM25 title score\n",
    "    print(\"\\tElasticsearch title field ...\")\n",
    "    res2 = search(\"clueweb12b\", query, \"title\", size=NUM_DOCS)\n",
    "    for doc in res2.get('hits', {}).get(\"hits\", {}):\n",
    "        doc_id = doc.get(\"_id\")\n",
    "        if doc_id in feats:\n",
    "            feats[doc_id][2] = doc.get(\"_score\")\n",
    "\n",
    "    # Feature 3: BM25 anchors score\n",
    "    # NOTE: we retrieve more candidate documents here\n",
    "    print(\"\\tElasticsearch anchors field ...\")\n",
    "    res3 = search(\"clueweb12b_anchors\", query, \"anchors\", size=NUM_DOCS*10)\n",
    "    for doc in res3.get('hits', {}).get(\"hits\", {}):\n",
    "        doc_id = doc.get(\"_id\")\n",
    "        if doc_id in feats:\n",
    "            feats[doc_id][3] = doc.get(\"_score\")\n",
    "                \n",
    "    # TODO: computation of additional features comes here \n",
    "                \n",
    "    # TODO: we can apply feature normalization here\n",
    "        \n",
    "    return feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Queries and qrels for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "QUERY_FILE = \"data/queries.txt\"\n",
    "QRELS_FILE = \"data/qrels.csv\"\n",
    "FEATURES_FILE = \"data/features.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "queries = load_queries(QUERY_FILE)\n",
    "qrels = load_qrels(QRELS_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the complete training data set (feature vectors and corresponding labels) and write it to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_training_data(queries, qrels, output_file):\n",
    "    with open(output_file, \"w\") as fout:\n",
    "        for qid, query in sorted(queries.items()):\n",
    "            # get feature vectors\n",
    "            feats = get_features(qid, query)\n",
    "            # assign target labels and write to file\n",
    "            for doc_id, feat in feats.items():\n",
    "                if doc_id in qrels[qid]: # we only consider docs where we have the target label\n",
    "                    rel = qrels[qid][doc_id]\n",
    "                    # NOTE: there shouldn't be \"missing\" features\n",
    "                    for fid in range(1, NUM_FEAT + 1):\n",
    "                        if fid not in feat:\n",
    "                            feat[fid] = 0  # default value for \"missing\" features\n",
    "                    # write to file\n",
    "                    feat_str = ['{}:{}'.format(k,v) for k,v in sorted(feat.items())]\n",
    "                    fout.write(\" \".join([str(rel), qid, doc_id] + feat_str) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_training_data(queries, qrels, FEATURES_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load training data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_X, train_y, qids, doc_ids = load_features(FEATURES_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model\n",
    "\n",
    "Set `max_depth` roughly to the square root of the number features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = RandomForestRegressor(max_depth=2, random_state=0)\n",
    "ltr = PointWiseLTRModel(clf)\n",
    "ltr._train(train_X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying model on unseen queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "QUERY2_FILE = \"data/queries2.txt\"\n",
    "FEATURES2_FILE = \"data/features2.txt\"\n",
    "OUTPUT_FILE = \"data/ltr2.txt\"\n",
    "TOP_DOCS = 20  # this many top docs to write to output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "queries2 = load_queries(QUERY2_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply model and write results to output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_format = \"trec\"\n",
    "\n",
    "with open(OUTPUT_FILE, \"w\") as fout:\n",
    "    for qid, query in sorted(queries2.items()):\n",
    "        # Get feature vectors\n",
    "        feats = get_features(qid, query)\n",
    "        \n",
    "        # Convert into the format required by the `PointWiseLTRModel` class\n",
    "        # and deal with missing feature values\n",
    "        doc_fts = []\n",
    "        doc_ids = []\n",
    "        \n",
    "        for doc_id, feat in feats.items():\n",
    "            for fid in range(1, NUM_FEAT + 1):\n",
    "                if fid not in feat:\n",
    "                    feat[fid] = -1\n",
    "            doc_fts.append(np.array([float(val) for fid, val in sorted(feat.items())]))\n",
    "            doc_ids.append(doc_id)\n",
    "        \n",
    "        # Get ranking\n",
    "        r = ltr.rank(doc_fts, doc_ids)    \n",
    "        # Write the results to file\n",
    "        rank = 1\n",
    "        for doc_id, score in r:\n",
    "            if rank <= TOP_DOCS:\n",
    "                if output_format == \"trec\":\n",
    "                    fout.write((\"\\t\".join([\"{}\"] * 6) + \"\\n\").format(qid, \"Q0\", doc_id, str(rank),\n",
    "                                                                 str(score), \"A3_3_Baseline\"))\n",
    "                else: \n",
    "                    fout.write(qid + \",\" + doc_id + \"\\n\")                            \n",
    "            rank += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
