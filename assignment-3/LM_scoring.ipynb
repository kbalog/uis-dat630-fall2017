{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 - LM retrieval\n",
    "\n",
    "Scoring documents using the Language Model (LM) approach, i.e., on a single field. In this example, we use JM smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import urllib\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "API = \"http://gustav1.ux.uis.no:5002\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BASIC_INDEX_NAME = \"clueweb12b\"\n",
    "ANCHORS_INDEX_NAME = \"clueweb12b_anchors\"\n",
    "\n",
    "def get_index_name(field):\n",
    "    return ANCHORS_INDEX_NAME if field == \"anchors\" else BASIC_INDEX_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "QUERY_FILE = \"data/queries.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some functions wrapping the API calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search(indexname, query, field, size=10):\n",
    "    url = \"/\".join([API, indexname, \"_search\"]) + \"?\" \\\n",
    "          + urllib.parse.urlencode({\"q\": query, \"df\": field, \"size\": size})\n",
    "    response = requests.get(url).text\n",
    "    \n",
    "    return json.loads(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def term_vectors(indexname, doc_id, term_statistics=False):\n",
    "    \"\"\"\n",
    "    param term_statistics: Boolean; True iff term_statistics are required.\n",
    "    \"\"\"\n",
    "    url = \"/\".join([API, indexname, doc_id, \"_termvectors\"]) + \"?\" \\\n",
    "          + urllib.parse.urlencode({\"term_statistics\": str(term_statistics).lower()})\n",
    "    response = requests.get(url).text\n",
    "\n",
    "    return json.loads(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def analyze(indexname, text):\n",
    "    \"\"\"\n",
    "    param text: string to analyze.\n",
    "    \"\"\"\n",
    "    query_terms = []\n",
    "\n",
    "    url = \"/\".join([API, indexname, \"_analyze\"]) + \"?\" + urllib.parse.urlencode({\"text\": text})\n",
    "    response = requests.get(url).text\n",
    "    tokens = json.loads(response).get(\"tokens\", [])\n",
    "    for t in sorted(tokens, key=lambda x: x[\"position\"]):\n",
    "        query_terms.append(t[\"token\"])\n",
    "\n",
    "    return query_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Document fields used for scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FIELDS = [\"title\", \"content\", \"anchors\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Smoothing: we use Jelinek-Mercer smoothing here with the following lambda parameter. (I.e., the same smoothing parameter is used for all fields.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LAMBDA = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Load the queries from the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_queries(query_file):\n",
    "    queries = {}\n",
    "    with open(query_file, \"r\") as fin:\n",
    "        for line in fin.readlines():\n",
    "            qid, query = line.strip().split(\" \", 1)\n",
    "            queries[qid] = query\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "queries = load_queries(QUERY_FILE)\n",
    "print(len(queries))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LM scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collection Language Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CollectionLM(object):\n",
    "    def __init__(self, qterms):\n",
    "        self._probs = {}\n",
    "        # computing P(t|C_i) for each field and for each query term\n",
    "        for field in FIELDS:\n",
    "            self._probs[field] = {}\n",
    "            for t in qterms:\n",
    "                self._probs[field][t] = self.__get_prob(field, t)\n",
    "        \n",
    "    def __get_prob(self, field, term):\n",
    "        # use a boolean query to find a document that contains the term\n",
    "        index_name = get_index_name(field)\n",
    "        hits = search(index_name, term, field, size=1).get(\"hits\", {}).get(\"hits\", {})\n",
    "        doc_id = hits[0][\"_id\"] if len(hits) > 0 else None\n",
    "        if doc_id is not None:\n",
    "            # ask for global term statistics when requesting the term vector of that doc (`term_statistics=True`)\n",
    "            \n",
    "            tv = term_vectors(index_name, doc_id, term_statistics=True)[\"term_vectors\"][field]\n",
    "            ttf = tv[\"terms\"].get(term, {}).get(\"ttf\", 0)  # total term count in the collection (in that field)\n",
    "            sum_ttf = tv[\"field_statistics\"][\"sum_ttf\"]\n",
    "            return ttf / sum_ttf\n",
    "        \n",
    "        return 0  # this only happens if none of the documents contain that term\n",
    "\n",
    "    def prob(self, field, term):\n",
    "        return self._probs.get(field, {}).get(term, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def score_lm(clm, qterms, doc_id, field):\n",
    "    score = 0  # log P(q|d)\n",
    "    \n",
    "    # Getting term frequency statistics for the given document field from Elasticsearch\n",
    "    # Note that global term statistics are not needed\n",
    "    index_name = get_index_name(field)\n",
    "    tv = term_vectors(index_name, doc_id).get(\"term_vectors\", {})\n",
    "\n",
    "    # compute field length $|d|$\n",
    "    len_d = 0  # document field length initialization\n",
    "    if field in tv:  # that document field may be NOT empty\n",
    "        len_d = sum([s[\"term_freq\"] for t, s in tv[field][\"terms\"].items()])\n",
    "        \n",
    "    # scoring the query\n",
    "    for t in qterms:\n",
    "        Pt_theta_d = 0  # P(t|\\theta_d)\n",
    "        if field in tv:\n",
    "            Pt_d = tv[field][\"terms\"].get(t, {}).get(\"term_freq\", 0) / len_d  # $P(t|d)$\n",
    "        else:  # that document field is empty\n",
    "            Pt_d = 0\n",
    "        Pt_C = clm.prob(field, t)  # $P(t|C)$\n",
    "        Pt_theta_d = (1 - LAMBDA) * Pt_d + LAMBDA * Pt_C  # $P(t|\\theta_{d})$ with J-M smoothing\n",
    "        score += math.log(Pt_theta_d) if Pt_theta_d > 0 else 0  # Pt_theta_d is 0 if t doesn't occur in any doc for that field, even with smoothing\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_queries(field):\n",
    "    index_name = get_index_name(field)\n",
    "    max_rank = 20\n",
    "\n",
    "    output_file = \"data/lm_jm_{}.runfile.txt\".format(field)\n",
    "    print(\"Outputting to {}\".format(output_file))\n",
    "    with open(output_file, \"w\") as fout:\n",
    "        fout.write(\"QueryId,DocumentId\\n\")  # header\n",
    "        for qid, query in sorted(queries.items()):\n",
    "            # get top 200 docs using BM25\n",
    "            print(\"\\tGet baseline ranking for [%s] '%s'\" % (qid, query))\n",
    "            res = search(index_name, query, field, size=200).get('hits', {})\n",
    "\n",
    "            # re-score docs using MLM\n",
    "            print(\"\\tRe-scoring documents using LM\")\n",
    "            # get analyzed query\n",
    "            qterms = analyze(index_name, query)\n",
    "            # get collection LM \n",
    "            # (this needs to be instantiated only once per query and can be used for scoring all documents)\n",
    "            clm = CollectionLM(qterms)        \n",
    "            scores = {}\n",
    "            for doc in res.get(\"hits\", {}):\n",
    "                doc_id = doc.get(\"_id\")\n",
    "                scores[doc_id] = score_lm(clm, qterms, doc_id, field)\n",
    "\n",
    "            # write top 20 results to file\n",
    "            for doc_id, score in sorted(scores.items(), key=lambda x: x[1], reverse=True)[:max_rank]:            \n",
    "                if rank <= max_rank:\n",
    "                    fout.write(qid + \",\" + doc_id + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for field in FIELDS:\n",
    "for field in [\"content\"]:\n",
    "    score_queries(field)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
