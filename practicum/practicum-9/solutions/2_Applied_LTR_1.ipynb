{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applied LTR, Part I.\n",
    "\n",
    "Based on the previous example, apply LTR using the queries and collection from Assignment 1. (The query and qrels files to use are [here](https://github.com/kbalog/uis-dat630-fall2017/tree/master/assignment-1/data).)\n",
    "\n",
    "1. **Extract features** for all document-query pairs from the qrels (i.e., all documents with relevance assessments).\n",
    "Use the following features (all are retrieval scores that you have computed for Assignment 1; we do not apply any normalization here):  \n",
    "  - BM25 retrieval score of the query against each field (title, content)\n",
    "  - LM retrieval score of the query against each field (title, content) using Jelinek-Mercer smoothing\n",
    "  - LM retrieval score of the query against each field (title, content) using Dirichlet smoothing\n",
    "\n",
    "2. **Train and evaluate the model using 5-fold cross-validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "INDEX_NAME = \"aquaint\"\n",
    "DOC_TYPE = \"doc\"\n",
    "\n",
    "es = Elasticsearch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "QUERY_FILE = \"data/queries.txt\"  # make sure the query file exists on this location\n",
    "QRELS_FILE = \"data/qrels2.csv\"  # file with the relevance judgments (ground truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FEATURES_FILE = \"data/features.txt\"  # output the features in this file\n",
    "OUTPUT_FILE = \"data/ltr.txt\"  # output the ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_queries(query_file):\n",
    "    queries = {}\n",
    "    with open(query_file, \"r\") as fin:\n",
    "        for line in fin.readlines():\n",
    "            qid, query = line.strip().split(\" \", 1)\n",
    "            queries[qid] = query\n",
    "    return queries\n",
    "\n",
    "queries = load_queries(QUERY_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load qrels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_qrels(qrels_file):\n",
    "    gt = {}  # holds a list of relevant documents for each queryID\n",
    "    with open(qrels_file, \"r\") as fin:\n",
    "        header = fin.readline().strip()\n",
    "        if header != \"queryID,docIDs\":\n",
    "            raise Exception(\"Incorrect file format!\")\n",
    "        for line in fin.readlines():\n",
    "            qid, docids = line.strip().split(\",\")\n",
    "            gt[qid] = docids.split()\n",
    "    return gt\n",
    "            \n",
    "qrels = load_qrels(QRELS_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1) Creating training data and writing it to a file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting features for query-document pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 6 features in total. Each feature here is a retrieval score, which we obtain using a different ES configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ES_CONFIG = {\n",
    "    1: {\n",
    "        \"field\": \"title\",\n",
    "        \"similarity\": {\n",
    "            \"default\": {\n",
    "                \"type\": \"BM25\", \n",
    "                \"b\": 0.75, \n",
    "                \"k1\": 1.2\n",
    "            } \n",
    "        }\n",
    "    },\n",
    "    2: {\n",
    "        \"field\": \"content\",\n",
    "        \"similarity\": {\n",
    "            \"default\": {\n",
    "                \"type\": \"BM25\", \n",
    "                \"b\": 0.75, \n",
    "                \"k1\": 1.2\n",
    "            } \n",
    "        }\n",
    "    },    \n",
    "    3: {\n",
    "        \"field\": \"title\",\n",
    "        \"similarity\": {\n",
    "            \"default\": {\n",
    "                \"type\": \"LMDirichlet\", \n",
    "                \"mu\": 200  # small for title\n",
    "            } \n",
    "        }\n",
    "    },\n",
    "    4: {\n",
    "        \"field\": \"content\",\n",
    "        \"similarity\": {\n",
    "            \"default\": {\n",
    "                \"type\": \"LMDirichlet\", \n",
    "                \"mu\": 2000  # larger for content\n",
    "            } \n",
    "        }\n",
    "    },\n",
    "    5: {\n",
    "        \"field\": \"title\",\n",
    "        \"similarity\": {\n",
    "            \"default\": {\n",
    "                \"type\": \"LMJelinekMercer\", \n",
    "                \"lambda\": 0.1  \n",
    "            } \n",
    "        }\n",
    "    },\n",
    "    6: {\n",
    "        \"field\": \"content\",\n",
    "        \"similarity\": {\n",
    "            \"default\": {\n",
    "                \"type\": \"LMJelinekMercer\", \n",
    "                \"lambda\": 0.1  \n",
    "            } \n",
    "        }\n",
    "    }    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collecting feature values in the `features` dict. It has the structure `features[qid][docid][fid] = value`, where fid is a feature ID (1..6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing values for feature # 1\n",
      "Computing values for feature # 2\n",
      "Computing values for feature # 3\n",
      "Computing values for feature # 4\n",
      "Computing values for feature # 5\n",
      "Computing values for feature # 6\n"
     ]
    }
   ],
   "source": [
    "features = {}\n",
    "\n",
    "for fid in range(1, len(ES_CONFIG) + 1):\n",
    "    print(\"Computing values for feature #\", fid)\n",
    "    # Set ES similarity config\n",
    "    es.indices.close(index=INDEX_NAME)\n",
    "    es.indices.put_settings(index=INDEX_NAME, body={\"similarity\": ES_CONFIG[fid][\"similarity\"]})\n",
    "    es.indices.open(index=INDEX_NAME)\n",
    "\n",
    "    time.sleep(1)  # wait until it takes effect\n",
    "\n",
    "    for qid, query in queries.items():\n",
    "        if qid not in features:\n",
    "            features[qid] = {}\n",
    "        #print(\"Ranking documents for [%s] '%s'\" % (qid, query))\n",
    "        res = es.search(index=INDEX_NAME, q=query, df=ES_CONFIG[fid][\"field\"], _source=False, size=1000).get('hits', {})\n",
    "        for doc in res.get(\"hits\", {}):\n",
    "            docid = doc.get(\"_id\")\n",
    "            if docid not in features[qid]:\n",
    "                features[qid][docid] = {}\n",
    "            features[qid][docid][fid] = doc.get(\"_score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking up relevance labels and writing training data to file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT** Here, we consider all documents that are retrieved in the top-1000 for any field or retrieval model. If the document is not in the qrels file than it'll be considered a negative training instance (target label=0). This leads to a very unbalanced training data set, with lot more negative than positive instances. It is your task to figure out how to deal with this issue (in Part II of the exerise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(FEATURES_FILE, \"w\") as fout:\n",
    "    for qid, query in queries.items():\n",
    "        for docid, ft in features[qid].items():\n",
    "            # Note that docid will not have a feature value for feature ID i\n",
    "            # if it was not retrieved in the top-1000 positions for that feature\n",
    "            # Here, we use -1 as the value for \"missing\" features\n",
    "            for fid in range(1, len(ES_CONFIG) + 1):\n",
    "                if fid not in ft:\n",
    "                    ft[fid] = -1\n",
    "            \n",
    "            # relevance label is determined based on the ground truth (qrels) file\n",
    "            label = 1 if docid in qrels.get(qid, []) else 0\n",
    "                        \n",
    "            feat_str = ['{}:{}'.format(k,v) for k,v in ft.items()]\n",
    "            fout.write(\" \".join([str(label), qid, docid] + feat_str) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2) Loading training data from file and performing retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning-to-rank code copy-pasted from the example (Task 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A class for pointwise-based learning to rank model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PointWiseLTRModel(object):\n",
    "    def __init__(self, regressor):\n",
    "        \"\"\"\n",
    "        :param classifier: an instance of scikit-learn regressor\n",
    "        \"\"\"\n",
    "        self.regressor = regressor\n",
    "\n",
    "    def _train(self, X, y):\n",
    "        \"\"\"\n",
    "        Trains and LTR model.\n",
    "        :param X: features of training instances\n",
    "        :param y: relevance assessments of training instances\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        assert self.regressor is not None\n",
    "        self.model = self.regressor.fit(X, y)\n",
    "\n",
    "    def rank(self, ft, doc_ids):\n",
    "        \"\"\"\n",
    "        Predicts relevance labels and rank documents for a given query\n",
    "        :param ft: a list of features for query-doc pairs\n",
    "        :param ft: a list of document ids\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        assert self.model is not None\n",
    "        rel_labels = self.model.predict(ft)\n",
    "        sort_indices = np.argsort(rel_labels)[::-1]\n",
    "\n",
    "        results = []\n",
    "        for i in sort_indices:\n",
    "            results.append((doc_ids[i], rel_labels[i]))\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read training data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data_from_file(path):\n",
    "    \"\"\"\n",
    "    :param path: path of file\n",
    "    :return: X features of data, y labels of data, group a list of numbers indicate how many instances for each query\n",
    "    \"\"\"\n",
    "    X, y, qids, doc_ids = [], [], [], []\n",
    "    with open(path, \"r\") as f:\n",
    "        i, s_qid = 0, None\n",
    "        for line in f:\n",
    "            items = line.strip().split()\n",
    "            label = int(items[0])\n",
    "            qid = items[1]\n",
    "            doc_id = items[2]\n",
    "            features = np.array([float(i.split(\":\")[1]) for i in items[3:]])\n",
    "            X.append(features)\n",
    "            y.append(label)\n",
    "            qids.append(qid)\n",
    "            doc_ids.append(doc_id)\n",
    "\n",
    "    return X, y, qids, doc_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, applying LTR for this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#queries:  50\n",
      "#query-doc pairs:  110185\n"
     ]
    }
   ],
   "source": [
    "X, y, qids, doc_ids = read_data_from_file(path=FEATURES_FILE)\n",
    "qids_unique= list(set(qids))\n",
    "\n",
    "print(\"#queries: \", len(qids_unique))\n",
    "print(\"#query-doc pairs: \", len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying 5-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold #7\n",
      "\tTraining model ...\n",
      "\tApplying model ...\n",
      "\t\tRanking docs for queryID 622\n",
      "\t\tRanking docs for queryID 419\n",
      "\t\tRanking docs for queryID 307\n",
      "\t\tRanking docs for queryID 375\n",
      "\t\tRanking docs for queryID 650\n",
      "\t\tRanking docs for queryID 354\n",
      "\t\tRanking docs for queryID 426\n",
      "\t\tRanking docs for queryID 389\n",
      "\t\tRanking docs for queryID 408\n",
      "\t\tRanking docs for queryID 303\n",
      "Fold #110185\n",
      "\tTraining model ...\n",
      "\tApplying model ...\n",
      "\t\tRanking docs for queryID 322\n",
      "\t\tRanking docs for queryID 448\n",
      "\t\tRanking docs for queryID 367\n",
      "\t\tRanking docs for queryID 436\n",
      "\t\tRanking docs for queryID 401\n",
      "\t\tRanking docs for queryID 314\n",
      "\t\tRanking docs for queryID 383\n",
      "\t\tRanking docs for queryID 394\n",
      "\t\tRanking docs for queryID 310\n",
      "\t\tRanking docs for queryID 435\n",
      "Fold #110185\n",
      "\tTraining model ...\n",
      "\tApplying model ...\n",
      "\t\tRanking docs for queryID 658\n",
      "\t\tRanking docs for queryID 347\n",
      "\t\tRanking docs for queryID 443\n",
      "\t\tRanking docs for queryID 651\n",
      "\t\tRanking docs for queryID 393\n",
      "\t\tRanking docs for queryID 374\n",
      "\t\tRanking docs for queryID 638\n",
      "\t\tRanking docs for queryID 689\n",
      "\t\tRanking docs for queryID 648\n",
      "\t\tRanking docs for queryID 353\n",
      "Fold #110185\n",
      "\tTraining model ...\n",
      "\tApplying model ...\n",
      "\t\tRanking docs for queryID 363\n",
      "\t\tRanking docs for queryID 416\n",
      "\t\tRanking docs for queryID 399\n",
      "\t\tRanking docs for queryID 427\n",
      "\t\tRanking docs for queryID 397\n",
      "\t\tRanking docs for queryID 341\n",
      "\t\tRanking docs for queryID 330\n",
      "\t\tRanking docs for queryID 372\n",
      "\t\tRanking docs for queryID 344\n",
      "\t\tRanking docs for queryID 325\n",
      "Fold #110185\n",
      "\tTraining model ...\n",
      "\tApplying model ...\n",
      "\t\tRanking docs for queryID 625\n",
      "\t\tRanking docs for queryID 409\n",
      "\t\tRanking docs for queryID 433\n",
      "\t\tRanking docs for queryID 439\n",
      "\t\tRanking docs for queryID 336\n",
      "\t\tRanking docs for queryID 345\n",
      "\t\tRanking docs for queryID 378\n",
      "\t\tRanking docs for queryID 639\n",
      "\t\tRanking docs for queryID 362\n",
      "\t\tRanking docs for queryID 404\n"
     ]
    }
   ],
   "source": [
    "FOLDS = 5\n",
    "\n",
    "fout = open(OUTPUT_FILE, \"w\")\n",
    "# write header\n",
    "fout.write(\"QueryId,DocumentId\\n\")\n",
    "    \n",
    "for f in range(FOLDS):\n",
    "    print(\"Fold #{}\".format(i + 1))\n",
    "    \n",
    "    train_qids, test_qids = [], []  # holds the IDs of train and test queries\n",
    "    train_ids, test_ids = [], []  # holds the instance IDs (indices in X )\n",
    "\n",
    "    for i in range(len(qids_unique)):\n",
    "        qid = qids_unique[i]\n",
    "        if i % FOLDS == f:  # test query\n",
    "            test_qids.append(qid)\n",
    "        else:  # train query\n",
    "            train_qids.append(qid)\n",
    "\n",
    "    train_X, train_y = [], []  # training feature values and target labels\n",
    "    test_X = []  # for testing we only have feature values\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        if qids[i] in train_qids:\n",
    "            train_X.append(X[i])\n",
    "            train_y.append(y[i])\n",
    "        else:\n",
    "            test_X.append(X[i])\n",
    "\n",
    "    # Create and train LTR model\n",
    "    print(\"\\tTraining model ...\")\n",
    "    clf = RandomForestRegressor(max_depth=3, random_state=0)\n",
    "    ltr = PointWiseLTRModel(clf)\n",
    "    ltr._train(train_X, train_y)\n",
    "    \n",
    "    # Apply LTR model on the remaining fold (test queries)\n",
    "    print(\"\\tApplying model ...\")\n",
    "    \n",
    "    for qid in set(test_qids):\n",
    "        print(\"\\t\\tRanking docs for queryID {}\".format(qid))\n",
    "        # Collect the features and docids for that (test) query `qid`\n",
    "        test_ft, test_docids = [], []\n",
    "        for i in range(len(X)):\n",
    "            if qids[i] == qid:\n",
    "                test_ft.append(X[i])\n",
    "                test_docids.append(doc_ids[i])\n",
    "        \n",
    "        # Get ranking\n",
    "        r = ltr.rank(test_ft, test_docids)    \n",
    "        # Write the results to file\n",
    "        for doc, score in r:\n",
    "            fout.write(qid + \",\" + doc + \"\\n\")\n",
    "        \n",
    "fout.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
